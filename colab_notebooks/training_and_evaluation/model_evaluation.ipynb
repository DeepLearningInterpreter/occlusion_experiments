{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model evaluation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningInterpreter/occlusion_experiments/blob/master/colab_notebooks/training_and_evaluation/model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vtui76m8NgAL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "This notebook can be used to evaluate the performance of trained detection models on the validation and the test set. "
      ]
    },
    {
      "metadata": {
        "id": "iBLb49moMRRr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is advisable to save the *events* files this notebook outputs, so that they can be accessed later through TensorBoard. For information about storing and loading external data with colab see: https://colab.research.google.com/notebooks/io.ipynb.  \n",
        "\n",
        "Set the variables below as you desire. Choose one of the trained models from the directory:  \"occlusion_experiments\\main_content\\multitude_of_possible_detectors\\training\". "
      ]
    },
    {
      "metadata": {
        "id": "jzOaESVJ2V8W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Choose the name of the model\n",
        "MODEL_NAME = \"FRCNN_ext_lr3\"\n",
        "#Choose on which set to evaluate performance \"validation\" or \"test\"\n",
        "SET = \"validation\"\n",
        "\n",
        "#The resulting event files will be saved in the MODEL_DIR\n",
        "MODEL_DIR=\"/content/occlusion_experiments/main_content/multitude_of_possible_detectors/detections/\" + MODEL_NAME\n",
        "#This variable points to the location of the pipeline_config file\n",
        "PIPELINE_CONFIG=\"/content/occlusion_experiments/main_content/multitude_of_possible_detectors/training/\" + MODEL_NAME + \"/pipeline_\" + SET + \".config\"\n",
        "#This variable points to the directory in which the model checkpoints are saved.\n",
        "CHECKPOINT_DIR=\"/content/occlusion_experiments/main_content/multitude_of_possible_detectors/training/\" + MODEL_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JnaC-s_3jaG6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------------------------------\n",
        "##Cloning the GitHub repository to the cloud server. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1Gl-A13XLHZc",
        "colab_type": "code",
        "outputId": "ec639097-c951-45a2-e8e8-b5ca0ec3868b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "cell_type": "code",
      "source": [
        "#Downloading and installing git lfs\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 72 not upgraded.\n",
            "Need to get 4,931 kB of archives.\n",
            "After this operation, 12.3 MB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 2.6.1 [4,931 kB]\n",
            "Fetched 4,931 kB in 1s (5,269 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 110855 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.6.1_amd64.deb ...\n",
            "Unpacking git-lfs (2.6.1) ...\n",
            "Setting up git-lfs (2.6.1) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Git LFS initialized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uh3_od2-zxdl",
        "colab_type": "code",
        "outputId": "c0600ca1-73c6-4e19-aaaf-8e685ef0aa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "#This may take a while.\n",
        "\n",
        "#Cloning repository. The exclude flag indicates that large files from the \"training\" and \"evaluation\" \n",
        "#subdirectories should not be downloaded\n",
        "!git lfs clone https://github.com/DeepLearningInterpreter/occlusion_experiments.git --exclude=\"occlusion_experiments/main_content/multitude_of_possible_detectors/frozen_models_for_detection, occlusion_experiments/main_content/multitude_of_possible_detectors/evaluation_outcomes\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "Cloning into 'occlusion_experiments'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 2067 (delta 44), reused 38 (delta 20), pack-reused 1998\u001b[K\n",
            "Receiving objects: 100% (2067/2067), 185.81 MiB | 28.37 MiB/s, done.\n",
            "Resolving deltas: 100% (321/321), done.\n",
            "Checking out files: 100% (2203/2203), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wOvyEGjO0bOT",
        "colab_type": "code",
        "outputId": "53348557-6d70-41ce-e5ea-82a8fcd56dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/occlusion_experiments/TF_object_detection_API_modified\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/occlusion_experiments/TF_object_detection_API_modified'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "0XyPrArYK5F_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Some installations"
      ]
    },
    {
      "metadata": {
        "id": "LtjUzEd14P91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "os.environ['PYTHONPATH'] += \":/content/occlusion_experiments/TF_object_detection_API_modified/slim\"\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tMLnMQ7bw5M",
        "colab_type": "code",
        "outputId": "29c059cd-7637-4cdf-eb92-8b031497746b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pycocotools"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_IuyXOMVE3a8",
        "colab_type": "code",
        "outputId": "03976f0e-d936-409c-9c4a-a6f395b7c554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/occlusion_experiments/TF_object_detection_API_modified/object_detection')\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/occlusion_experiments/TF_object_detection_API_modified/object_detection'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "4YyBDo2a3Uwt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------\n",
        "##Begin inference\n",
        "Running the cell below starts the evaluation. The resulting event files will be stored in the *CHECKPOINT_DIR* directory. The results of the evaluation can be accessed through tensorboard."
      ]
    },
    {
      "metadata": {
        "id": "eBANEdAbwMMu",
        "colab_type": "code",
        "outputId": "e77425f8-774f-41b0-a076-7cb99f395718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2454
        }
      },
      "cell_type": "code",
      "source": [
        "!python model_main.py \\\n",
        "    --model_dir=$MODEL_DIR \\\n",
        "    --pipeline_config_path=$PIPELINE_CONFIG \\\n",
        "    --checkpoint_dir=$CHECKPOINT_DIR \\\n",
        "    --run_once=True \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0124 14:55:53.980638 140683167487872 tf_logging.py:125] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0124 14:55:53.980884 140683167487872 tf_logging.py:115] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0124 14:55:53.981013 140683167487872 tf_logging.py:115] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0124 14:55:53.981156 140683167487872 tf_logging.py:115] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0124 14:55:53.981279 140683167487872 tf_logging.py:115] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0124 14:55:53.981412 140683167487872 tf_logging.py:115] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0124 14:55:53.982056 140683167487872 tf_logging.py:125] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0124 14:55:53.982221 140683167487872 tf_logging.py:115] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/content/occlusion_experiments/main_content/multitude_of_possible_detectors/detections/FRCNN_ext_lr3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff2ef676588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0124 14:55:53.982680 140683167487872 tf_logging.py:115] Using config: {'_model_dir': '/content/occlusion_experiments/main_content/multitude_of_possible_detectors/detections/FRCNN_ext_lr3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff2ef676588>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7ff2ef681400>) includes params argument, but params are not passed to Estimator.\n",
            "W0124 14:55:53.982951 140683167487872 tf_logging.py:125] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7ff2ef681400>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Writing pipeline config file to /content/occlusion_experiments/main_content/multitude_of_possible_detectors/detections/FRCNN_ext_lr3/pipeline.config\n",
            "I0124 14:55:53.984648 140683167487872 tf_logging.py:115] Writing pipeline config file to /content/occlusion_experiments/main_content/multitude_of_possible_detectors/detections/FRCNN_ext_lr3/pipeline.config\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0124 14:55:54.194179 140683167487872 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0124 14:55:54.384062 140683167487872 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "W0124 14:55:55.107575 140683167487872 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/builders/dataset_builder.py:148: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0124 14:55:55.143803 140683167487872 tf_logging.py:115] Calling model_fn.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0124 14:55:57.448176 140683167487872 tf_logging.py:115] Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0124 14:55:57.467531 140683167487872 tf_logging.py:115] Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0124 14:55:57.467893 140683167487872 tf_logging.py:115] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0124 14:55:58.853035 140683167487872 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/predictors/heads/box_head.py:93: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0124 14:55:58.861524 140683167487872 tf_logging.py:115] Scale of 0 disables regularizer.\n",
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "I0124 14:55:58.887429 140683167487872 tf_logging.py:115] Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:340: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0124 14:55:59.922955 140683167487872 tf_logging.py:125] From /usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/core/losses.py:340: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0124 14:56:02.246393 140683167487872 tf_logging.py:115] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-01-24-14:56:02\n",
            "I0124 14:56:02.275360 140683167487872 tf_logging.py:115] Starting evaluation at 2019-01-24-14:56:02\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0124 14:56:03.334897 140683167487872 tf_logging.py:115] Graph was finalized.\n",
            "2019-01-24 14:56:03.506054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-01-24 14:56:03.506476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-01-24 14:56:03.506515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-24 14:56:04.443680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-24 14:56:04.443749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-24 14:56:04.443773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-24 14:56:04.444156: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-01-24 14:56:04.444256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from /content/occlusion_experiments/main_content/multitude_of_possible_detectors/training/FRCNN_ext_lr3/model.ckpt-50000\n",
            "I0124 14:56:04.448964 140683167487872 tf_logging.py:115] Restoring parameters from /content/occlusion_experiments/main_content/multitude_of_possible_detectors/training/FRCNN_ext_lr3/model.ckpt-50000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0124 14:56:05.055114 140683167487872 tf_logging.py:115] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0124 14:56:05.132784 140683167487872 tf_logging.py:115] Done running local_init_op.\n",
            "2019-01-24 14:56:17.125850: W tensorflow/core/lib/png/png_io.cc:87] PNG warning: iCCP: known incorrect sRGB profile\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0124 14:57:24.988650 140681221424896 tf_logging.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0124 14:57:24.989389 140681221424896 tf_logging.py:115] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.12s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.03s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.573\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.769\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.634\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.282\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.602\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.539\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.622\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.622\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.306\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.652\n",
            "INFO:tensorflow:Finished evaluation at 2019-01-24-14:57:58\n",
            "I0124 14:57:58.259716 140683167487872 tf_logging.py:115] Finished evaluation at 2019-01-24-14:57:58\n",
            "INFO:tensorflow:Saving dict for global step 50000: DetectionBoxes_Precision/mAP = 0.57338494, DetectionBoxes_Precision/mAP (large) = 0.6023137, DetectionBoxes_Precision/mAP (medium) = 0.28197154, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.76892823, DetectionBoxes_Precision/mAP@.75IOU = 0.63367844, DetectionBoxes_Recall/AR@1 = 0.53945947, DetectionBoxes_Recall/AR@10 = 0.62216216, DetectionBoxes_Recall/AR@100 = 0.62216216, DetectionBoxes_Recall/AR@100 (large) = 0.652071, DetectionBoxes_Recall/AR@100 (medium) = 0.30625, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.22938654, Loss/BoxClassifierLoss/localization_loss = 0.13752522, Loss/RPNLoss/localization_loss = 0.19576958, Loss/RPNLoss/objectness_loss = 0.12746039, Loss/total_loss = 0.6901416, global_step = 50000, learning_rate = 2e-05, loss = 0.6901416\n",
            "I0124 14:57:58.260240 140683167487872 tf_logging.py:115] Saving dict for global step 50000: DetectionBoxes_Precision/mAP = 0.57338494, DetectionBoxes_Precision/mAP (large) = 0.6023137, DetectionBoxes_Precision/mAP (medium) = 0.28197154, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.76892823, DetectionBoxes_Precision/mAP@.75IOU = 0.63367844, DetectionBoxes_Recall/AR@1 = 0.53945947, DetectionBoxes_Recall/AR@10 = 0.62216216, DetectionBoxes_Recall/AR@100 = 0.62216216, DetectionBoxes_Recall/AR@100 (large) = 0.652071, DetectionBoxes_Recall/AR@100 (medium) = 0.30625, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.22938654, Loss/BoxClassifierLoss/localization_loss = 0.13752522, Loss/RPNLoss/localization_loss = 0.19576958, Loss/RPNLoss/objectness_loss = 0.12746039, Loss/total_loss = 0.6901416, global_step = 50000, learning_rate = 2e-05, loss = 0.6901416\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 50000: /content/occlusion_experiments/main_content/multitude_of_possible_detectors/training/FRCNN_ext_lr3/model.ckpt-50000\n",
            "I0124 14:57:59.979097 140683167487872 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 50000: /content/occlusion_experiments/main_content/multitude_of_possible_detectors/training/FRCNN_ext_lr3/model.ckpt-50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U9ZI6qOnR-CJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}